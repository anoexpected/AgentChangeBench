import math
import re
import statistics
from typing import Optional

import numpy as np
import pandas as pd
from loguru import logger
from pydantic import BaseModel

from tau2.data_model.message import Message, ToolMessage, AssistantMessage, UserMessage
from tau2.data_model.simulation import Results, SimulationRun


def is_successful(reward: float) -> bool:
    """
    Check if the reward is successful.
    """
    return (1 - 1e-6) <= reward <= (1 + 1e-6)


def safe_float(val, default: float = 0.0) -> float:
    """Convert value safely to float."""
    if val is None:
        return default
    try:
        return float(val)
    except (ValueError, TypeError):
        return default


def normalized_params(params: dict) -> tuple:
    """Recursively normalize parameters for consistency."""
    if not isinstance(params, dict):
        return str(params)

    normalized_items = []
    for k, v in sorted(params.items()):
        if isinstance(v, dict):
            normalized_items.append((k, normalized_params(v)))
        elif isinstance(v, list):
            try:
                sorted_v = tuple(sorted(v))
            except TypeError:
                sorted_v = tuple(str(item) for item in v)
            normalized_items.append((k, sorted_v))
        else:
            normalized_items.append((k, v))
    return tuple(normalized_items)


def extract_tool_calls_from_messages(messages: list[Message]) -> list[dict]:
    """
    Extract tool call metrics from tau2 message objects.

    Determines correctness and parameter validity according to AgentChangeBench specification:
    - T_correct: Tool executed without errors
    - P_params: Tool called with valid & optimal parameters
    """
    tool_calls = []

    # Create mapping of tool call IDs to their results
    tool_results = {}
    for msg in messages:
        if isinstance(msg, ToolMessage):
            # Tool message contains the result of a tool call
            tool_results[msg.id] = {
                "success": not msg.error,
                "error": msg.error,
                "content": msg.content,
            }

    for msg in messages:
        # Look for tool calls in AssistantMessage and UserMessage objects
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            for tool_call in msg.tool_calls:
                # Determine correctness based on whether the tool executed without errors
                correct = True
                if hasattr(tool_call, "id") and tool_call.id in tool_results:
                    correct = tool_results[tool_call.id]["success"]

                # Determine parameter validity
                # For now, assume parameters are valid if:
                # 1. The tool call was attempted (it exists)
                # 2. The tool didn't fail due to parameter issues
                params_valid = True
                if hasattr(tool_call, "id") and tool_call.id in tool_results:
                    result = tool_results[tool_call.id]
                    if result["error"] and result.get("content"):
                        # Check if error message indicates parameter issues
                        error_content = str(result["content"]).lower()
                        if any(
                            keyword in error_content
                            for keyword in [
                                "invalid parameter",
                                "missing parameter",
                                "parameter error",
                                "bad parameter",
                                "invalid argument",
                                "missing argument",
                            ]
                        ):
                            params_valid = False

                # Calculate latency if timestamps are available
                latency = 0.0
                if (
                    hasattr(msg, "timestamp")
                    and hasattr(tool_call, "id")
                    and tool_call.id in tool_results
                ):
                    # Could implement timestamp-based latency calculation here
                    pass

                tool_call_metric = {
                    "name": tool_call.name,
                    "params": tool_call.arguments,  # ToolCall uses 'arguments' not 'params'
                    "cost": safe_float(msg.cost) if hasattr(msg, "cost") else 0.0,
                    "latency": latency,
                    "correct": correct,
                    "params_valid": params_valid,
                }
                tool_calls.append(tool_call_metric)

    return tool_calls


# Old compute_tsr function removed - now using enhanced TSR implementation


# Old compute_tcrr function removed - now using enhanced TCRR implementation


# Old compute_tue function removed - now using enhanced TUE implementation


def extract_domain_from_task_id(task_id: str) -> str:
    """Extract domain name from task_id."""
    parts = task_id.split("_")
    if len(parts) > 0:
        return parts[0]
    return "unknown"


class AgentMetrics(BaseModel):
    avg_reward: float
    pass_hat_ks: dict[int, float]
    avg_agent_cost: float
    # New AgentChangeBench metrics
    tsr: float  # Task Success Rate
    tue: float  # Tool Usage Efficiency
    tcrr: float  # Tool-Call Redundancy Ratio
    num_tool_calls: int
    # Enhanced TSR metrics (multi-channel breakdown)
    tsr_communicate_info: float = 0.0
    tsr_action: float = 0.0
    tsr_nl_assertion: float = 0.0
    tsr_weights: dict = {}  # Weights used in TSR calculation
    tsr_by_task: dict = {}  # Task-level TSR breakdown
    # Enhanced TUE metrics
    tue_tool_correctness: float = 0.0
    tue_param_accuracy: float = 0.0
    tue_correct_calls: int = 0
    tue_valid_param_calls: int = 0
    tue_by_task: dict = {}  # Task-level TUE breakdown
    # Enhanced TCRR metrics
    tcrr_window_size: int = 3
    tcrr_total_calls: int = 0
    tcrr_redundant_calls: int = 0
    tcrr_by_task: dict = {}  # Task-level TCRR breakdown
    # Enhanced GSRT metrics
    gsrt_median_ack: Optional[float] = None
    gsrt_median_tool: Optional[float] = None
    gsrt_median_outcome: Optional[float] = None
    gsrt_num_shifts: int = 0
    gsrt_recovery_rate: float = 0.0
    gsrt_transfer_rate: float = 0.0
    gsrt_never_recovered_rate: float = 0.0
    gsrt_by_task: dict = {}  # Task-level breakdown

    def as_dict(self) -> dict:
        data = {
            "avg_reward": self.avg_reward,
            "avg_agent_cost": self.avg_agent_cost,
            "tsr": self.tsr,
            "tue": self.tue,
            "tcrr": self.tcrr,
            "num_tool_calls": self.num_tool_calls,
            "tsr_communicate_info": self.tsr_communicate_info,
            "tsr_action": self.tsr_action,
            "tsr_nl_assertion": self.tsr_nl_assertion,
            "tsr_weights": self.tsr_weights,
            "tsr_by_task": self.tsr_by_task,
            "tue_tool_correctness": self.tue_tool_correctness,
            "tue_param_accuracy": self.tue_param_accuracy,
            "tue_correct_calls": self.tue_correct_calls,
            "tue_valid_param_calls": self.tue_valid_param_calls,
            "tue_by_task": self.tue_by_task,
            "tcrr_window_size": self.tcrr_window_size,
            "tcrr_total_calls": self.tcrr_total_calls,
            "tcrr_redundant_calls": self.tcrr_redundant_calls,
            "tcrr_by_task": self.tcrr_by_task,
            "gsrt_median_ack": self.gsrt_median_ack,
            "gsrt_median_tool": self.gsrt_median_tool,
            "gsrt_median_outcome": self.gsrt_median_outcome,
            "gsrt_num_shifts": self.gsrt_num_shifts,
            "gsrt_recovery_rate": self.gsrt_recovery_rate,
            "gsrt_transfer_rate": self.gsrt_transfer_rate,
            "gsrt_never_recovered_rate": self.gsrt_never_recovered_rate,
            "gsrt_by_task": self.gsrt_by_task,
        }
        for k, v in self.pass_hat_ks.items():
            data[f"pass_hat_{k}"] = v
        return data


def pass_hat_k(num_trials: int, success_count: int, k: int) -> float:
    """
    Compute the pass^k metric for the given number of trials, success count, and k.
    from https://arxiv.org/pdf/2406.12045
    Args:
        num_trials: The number of trials.
        success_count: The number of successful trials.
        k: The number of trials to consider.
    Returns:
        The pass^k metric.
    """
    if num_trials < k:
        raise ValueError(f"Number of trials {num_trials} is less than k {k}.")
    return math.comb(success_count, k) / math.comb(num_trials, k)


def get_metrics_df(results: Results) -> tuple[pd.DataFrame, int]:
    """
    Convert the results to a dataframe and add a column for success.
    Checks that all simulations have the same number of trials.
    Returns the maximum number of trials that can be used for pass^k metrics.
    """
    df = results.to_df()
    df["success"] = df.reward.apply(is_successful)
    if len(df.info_num_trials.unique()) > 1:
        logger.warning(
            f"All simulations must have the same number of trials. Found {df.info_num_trials.unique()}"
        )
    max_k = df.info_num_trials.max()

    task_ids_counts = [(tid, count) for tid, count in df.task_id.value_counts().items()]
    task_ids_counts.sort(key=lambda x: x[1])
    min_k = task_ids_counts[0][1]
    if min_k < max_k:
        logger.warning(
            f"The minimum number of trials for a task is {min_k}, which is less than the expected number of trials {max_k}. Setting max k to {min_k}."
        )
        max_k = min_k
    return df, max_k


def compute_metrics_simple(results: Results) -> AgentMetrics:
    """
    Compute metrics for the agent with simpler approach that doesn't rely on task evaluation criteria.
    - average reward
    - TSR (Task Success Rate)
    - TUE (Tool Usage Efficiency)
    - TCRR (Tool-Call Redundancy Ratio)
    - GSRT (Goal Shift Recovery Time)
    """
    # Simple reward calculation
    rewards = [
        sim.reward_info.reward if sim.reward_info else 0.0
        for sim in results.simulations
    ]
    avg_reward = np.mean(rewards) if rewards else 0.0

    # Simple cost calculation
    agent_costs = [
        sim.agent_cost if sim.agent_cost else 0.0 for sim in results.simulations
    ]
    avg_agent_cost = np.mean(agent_costs) if agent_costs else 0.0

    # Pass^k metrics - simplified
    pass_hat_ks = {}
    if results.simulations:
        # Group by task_id and compute success rates
        task_groups = {}
        for sim in results.simulations:
            task_id = sim.task_id
            if task_id not in task_groups:
                task_groups[task_id] = []
            task_groups[task_id].append(
                is_successful(sim.reward_info.reward if sim.reward_info else 0.0)
            )

        # Compute pass^k for k=1 (basic success rate)
        if task_groups:
            task_success_rates = []
            for task_id, successes in task_groups.items():
                success_rate = sum(successes) / len(successes)
                task_success_rates.append(success_rate)
            pass_hat_ks[1] = np.mean(task_success_rates)

    # Compute enhanced TSR metrics with multi-channel breakdown
    try:
        from tau2.metrics.tsr import compute_tsr_enhanced
        
        tsr_result = compute_tsr_enhanced(results.tasks, results.simulations)
        tsr = tsr_result.overall_tsr
        tsr_communicate_info = tsr_result.communicate_info.success_rate
        tsr_action = tsr_result.action.success_rate
        tsr_nl_assertion = tsr_result.nl_assertion.success_rate
        tsr_weights = tsr_result.weights
        tsr_by_task = tsr_result.by_task
        
    except Exception as e:
        logger.warning(f"Enhanced TSR computation failed, falling back to simple: {e}")
        # Fallback to simple TSR calculation
        success_count = sum(1 for sim in results.simulations 
                          if sim.reward_info and safe_float(sim.reward_info.reward) > 0)
        tsr = success_count / len(results.simulations) if results.simulations else 0.0
        tsr_communicate_info = 0.0
        tsr_action = 0.0
        tsr_nl_assertion = 0.0
        tsr_weights = {"communicate_info": 0.5, "action": 0.3, "nl_assertion": 0.2}
        tsr_by_task = {}

    # Extract all tool calls
    all_tool_calls = []
    for run in results.simulations:
        tool_calls = extract_tool_calls_from_messages(run.messages)
        all_tool_calls.extend(tool_calls)

    num_tool_calls = len(all_tool_calls)

    if all_tool_calls:
        # Calculate enhanced TCRR with window-based approach
        try:
            from tau2.metrics.tcrr import compute_tcrr_enhanced, compute_tcrr_by_task
            
            tcrr_result = compute_tcrr_enhanced(results.simulations, window_size=3)
            tcrr = tcrr_result.redundancy_ratio
            tcrr_window_size = tcrr_result.window_size
            tcrr_total_calls = tcrr_result.total_calls
            tcrr_redundant_calls = tcrr_result.redundant_calls
            
            # Get task-level breakdown
            tcrr_by_task_results = compute_tcrr_by_task(results.simulations, window_size=3)
            tcrr_by_task = {
                task_id: {
                    "redundancy_ratio": result.redundancy_ratio,
                    "total_calls": result.total_calls,
                    "redundant_calls": result.redundant_calls,
                }
                for task_id, result in tcrr_by_task_results.items()
            }
        except Exception as e:
            logger.warning(f"Enhanced TCRR computation failed, falling back to simple: {e}")
            # Fallback to simple TCRR calculation
            seen_calls = set()
            duplicate_count = 0
            for call in all_tool_calls:
                try:
                    params_norm = normalized_params(call["params"])
                    identity = (call["name"], params_norm)
                    if identity in seen_calls:
                        duplicate_count += 1
                    else:
                        seen_calls.add(identity)
                except Exception:
                    identity = (call["name"], str(call["params"]))
                    if identity in seen_calls:
                        duplicate_count += 1
                    else:
                        seen_calls.add(identity)
            tcrr = duplicate_count / len(all_tool_calls) if all_tool_calls else 0.0
            tcrr_window_size = 3
            tcrr_total_calls = len(all_tool_calls)
            tcrr_redundant_calls = int(tcrr * len(all_tool_calls))
            tcrr_by_task = {}

        # Calculate enhanced TUE without latency component
        try:
            from tau2.metrics.tue import compute_tue_enhanced_for_simulations, compute_tue_by_task
            
            tue_result = compute_tue_enhanced_for_simulations(results.simulations)
            tue = tue_result.overall_tue
            tue_tool_correctness = tue_result.tool_correctness
            tue_param_accuracy = tue_result.param_accuracy
            tue_correct_calls = tue_result.correct_calls
            tue_valid_param_calls = tue_result.valid_param_calls
            
            # Get task-level breakdown
            tue_by_task_results = compute_tue_by_task(results.simulations)
            tue_by_task = {
                task_id: {
                    "overall_tue": result.overall_tue,
                    "tool_correctness": result.tool_correctness,
                    "param_accuracy": result.param_accuracy,
                    "total_calls": result.total_calls,
                    "correct_calls": result.correct_calls,
                    "valid_param_calls": result.valid_param_calls,
                }
                for task_id, result in tue_by_task_results.items()
            }
        except Exception as e:
            logger.warning(f"Enhanced TUE computation failed, falling back to simple: {e}")
            # Fallback to original TUE calculation
            all_costs = [call["cost"] for call in all_tool_calls if call["cost"] > 0]
            all_latencies = [call["latency"] for call in all_tool_calls if call["latency"] > 0]
            cost_cap = np.percentile(all_costs, 95) if all_costs else 1.0
            latency_cap = np.percentile(all_latencies, 95) if all_latencies else 1.0
            cost_cap = max(cost_cap, 0.001)
            latency_cap = max(latency_cap, 0.001)
            # Fallback to simple TUE calculation
            num_total = len(all_tool_calls)
            num_correct = sum(1 for call in all_tool_calls if call.get("correct", False))
            num_valid_params = sum(1 for call in all_tool_calls if call.get("params_valid", False))
            T_correct = num_correct / num_total
            P_params = num_valid_params / num_total
            total_cost = sum(call.get("cost", 0.0) for call in all_tool_calls)
            C_cost = max(0.0, min(1.0, 1.0 - (total_cost / cost_cap))) if cost_cap > 0 else 1.0
            valid_latencies = [call.get("latency", 0.0) for call in all_tool_calls if call.get("latency", 0.0) > 0]
            if valid_latencies and latency_cap > 0:
                avg_latency = sum(valid_latencies) / len(valid_latencies)
                capped_latency = min(avg_latency, latency_cap)
                L_latency = max(0.0, min(1.0, 1.0 - (capped_latency / latency_cap)))
            else:
                L_latency = 1.0
            tue = 0.4 * T_correct + 0.25 * P_params + 0.2 * C_cost + 0.15 * L_latency
            
            # Extract basic metrics for fallback
            tue_correct_calls = sum(1 for call in all_tool_calls if call.get("correct", False))
            tue_valid_param_calls = sum(1 for call in all_tool_calls if call.get("params_valid", False))
            tue_tool_correctness = tue_correct_calls / len(all_tool_calls) if all_tool_calls else 0.0
            tue_param_accuracy = tue_valid_param_calls / len(all_tool_calls) if all_tool_calls else 0.0
            tue_by_task = {}
    else:
        tcrr = 0.0
        tue = 0.0
        tcrr_window_size = 3
        tcrr_total_calls = 0
        tcrr_redundant_calls = 0
        tcrr_by_task = {}
        tue_tool_correctness = 0.0
        tue_param_accuracy = 0.0
        tue_correct_calls = 0
        tue_valid_param_calls = 0
        tue_by_task = {}

    # Compute enhanced GSRT metrics
    try:
        from tau2.metrics.gsrt import compute_gsrt_enhanced_metrics

        judge_model = getattr(results.info, "gsrt_judge_llm", None) or "gpt-4o-mini"
        judge_args = getattr(results.info, "gsrt_judge_llm_args", None) or {
            "temperature": 0.0
        }
        
        gsrt_aggregate = compute_gsrt_enhanced_metrics(
            results.tasks, results.simulations, judge_model, judge_args
        )
        
        gsrt_median_ack = gsrt_aggregate.aggregate_median_ack
        gsrt_median_tool = gsrt_aggregate.aggregate_median_tool
        gsrt_median_outcome = gsrt_aggregate.aggregate_median_outcome
        gsrt_num_shifts = gsrt_aggregate.total_shifts
        gsrt_recovery_rate = gsrt_aggregate.overall_recovery_rate
        gsrt_transfer_rate = gsrt_aggregate.overall_transfer_rate
        gsrt_never_recovered_rate = gsrt_aggregate.never_recovered_rate
        gsrt_by_task = {
            task_id: {
                "median_ack": task_result.median_gsrt_ack,
                "median_tool": task_result.median_gsrt_tool,
                "median_outcome": task_result.median_gsrt_outcome,
                "total_shifts": task_result.total_shifts,
                "recovery_rate": task_result.recovery_rate,
                "transfer_rate": task_result.transfer_rate,
            }
            for task_id, task_result in gsrt_aggregate.by_task.items()
        }
    except Exception as e:
        logger.warning(f"Enhanced GSRT computation failed: {e}")
        gsrt_median_ack, gsrt_median_tool, gsrt_median_outcome = None, None, None
        gsrt_num_shifts = 0
        gsrt_recovery_rate, gsrt_transfer_rate, gsrt_never_recovered_rate = 0.0, 0.0, 0.0
        gsrt_by_task = {}

    return AgentMetrics(
        avg_reward=avg_reward,
        pass_hat_ks=pass_hat_ks,
        avg_agent_cost=avg_agent_cost,
        tsr=tsr,
        tue=tue,
        tcrr=tcrr,
        num_tool_calls=num_tool_calls,
        tsr_communicate_info=tsr_communicate_info,
        tsr_action=tsr_action,
        tsr_nl_assertion=tsr_nl_assertion,
        tsr_weights=tsr_weights,
        tsr_by_task=tsr_by_task,
        tcrr_window_size=tcrr_window_size,
        tcrr_total_calls=tcrr_total_calls,
        tcrr_redundant_calls=tcrr_redundant_calls,
        tcrr_by_task=tcrr_by_task,
        tue_tool_correctness=tue_tool_correctness,
        tue_param_accuracy=tue_param_accuracy,
        tue_correct_calls=tue_correct_calls,
        tue_valid_param_calls=tue_valid_param_calls,
        tue_by_task=tue_by_task,
        gsrt_median_ack=gsrt_median_ack,
        gsrt_median_tool=gsrt_median_tool,
        gsrt_median_outcome=gsrt_median_outcome,
        gsrt_num_shifts=gsrt_num_shifts,
        gsrt_recovery_rate=gsrt_recovery_rate,
        gsrt_transfer_rate=gsrt_transfer_rate,
        gsrt_never_recovered_rate=gsrt_never_recovered_rate,
        gsrt_by_task=gsrt_by_task,
    )


def compute_metrics(results: Results) -> AgentMetrics:
    """
    Compute metrics for the agent.
    - average reward
    - pass^k
    - TSR (Task Success Rate)
    - TUE (Tool Usage Efficiency)
    - TCRR (Tool-Call Redundancy Ratio)
    - GSRT (Goal Shift Recovery Time via LLM-judged v2)

    Falls back to simpler computation if task evaluation criteria are missing.
    """
    try:
        # Try the full computation first
        df, df_pass_hat_k = get_metrics_df(results)
        avg_reward = df.reward.mean()
        pass_hat_ks = {}
        for column in df_pass_hat_k.columns:
            if match := re.match(r"pass\^(\d+)", column):
                k = int(match.group(1))
                pass_hat_ks[k] = df_pass_hat_k[column].mean()
        avg_agent_cost = df.agent_cost.mean()

        # Compute enhanced TSR metrics with multi-channel breakdown
        try:
            from tau2.metrics.tsr import compute_tsr_enhanced
            
            tsr_result = compute_tsr_enhanced(results.tasks, results.simulations)
            tsr = tsr_result.overall_tsr
            tsr_communicate_info = tsr_result.communicate_info.success_rate
            tsr_action = tsr_result.action.success_rate
            tsr_nl_assertion = tsr_result.nl_assertion.success_rate
            tsr_weights = tsr_result.weights
            tsr_by_task = tsr_result.by_task
            
        except Exception as e:
            logger.warning(f"Enhanced TSR computation failed, falling back to simple: {e}")
            # Fallback to simple TSR calculation
            success_count = sum(1 for sim in results.simulations 
                              if sim.reward_info and safe_float(sim.reward_info.reward) > 0)
            tsr = success_count / len(results.simulations) if results.simulations else 0.0
            tsr_communicate_info = 0.0
            tsr_action = 0.0
            tsr_nl_assertion = 0.0
            tsr_weights = {"communicate_info": 0.5, "action": 0.3, "nl_assertion": 0.2}
            tsr_by_task = {}

        # Extract all tool calls
        all_tool_calls = []
        for run in results.simulations:
            tool_calls = extract_tool_calls_from_messages(run.messages)
            all_tool_calls.extend(tool_calls)

        num_tool_calls = len(all_tool_calls)

        if all_tool_calls:
            # Calculate enhanced TCRR with window-based approach
            try:
                from tau2.metrics.tcrr import compute_tcrr_enhanced, compute_tcrr_by_task
                
                tcrr_result = compute_tcrr_enhanced(results.simulations, window_size=3)
                tcrr = tcrr_result.redundancy_ratio
                tcrr_window_size = tcrr_result.window_size
                tcrr_total_calls = tcrr_result.total_calls
                tcrr_redundant_calls = tcrr_result.redundant_calls
                
                # Get task-level breakdown
                tcrr_by_task_results = compute_tcrr_by_task(results.simulations, window_size=3)
                tcrr_by_task = {
                    task_id: {
                        "redundancy_ratio": result.redundancy_ratio,
                        "total_calls": result.total_calls,
                        "redundant_calls": result.redundant_calls,
                    }
                    for task_id, result in tcrr_by_task_results.items()
                }
            except Exception as e:
                logger.warning(f"Enhanced TCRR computation failed, falling back to simple: {e}")
                # Fallback to simple TCRR calculation
                seen_calls = set()
                duplicate_count = 0
                for call in all_tool_calls:
                    try:
                        params_norm = normalized_params(call["params"])
                        identity = (call["name"], params_norm)
                        if identity in seen_calls:
                            duplicate_count += 1
                        else:
                            seen_calls.add(identity)
                    except Exception:
                        identity = (call["name"], str(call["params"]))
                        if identity in seen_calls:
                            duplicate_count += 1
                        else:
                            seen_calls.add(identity)
                tcrr = duplicate_count / len(all_tool_calls) if all_tool_calls else 0.0
                tcrr_window_size = 3
                tcrr_total_calls = len(all_tool_calls)
                tcrr_redundant_calls = int(tcrr * len(all_tool_calls))
                tcrr_by_task = {}

            # Calculate enhanced TUE without latency component
            try:
                from tau2.metrics.tue import compute_tue_enhanced_for_simulations, compute_tue_by_task
                
                tue_result = compute_tue_enhanced_for_simulations(results.simulations)
                tue = tue_result.overall_tue
                tue_tool_correctness = tue_result.tool_correctness
                tue_param_accuracy = tue_result.param_accuracy
                tue_correct_calls = tue_result.correct_calls
                tue_valid_param_calls = tue_result.valid_param_calls
                
                # Get task-level breakdown
                tue_by_task_results = compute_tue_by_task(results.simulations)
                tue_by_task = {
                    task_id: {
                        "overall_tue": result.overall_tue,
                        "tool_correctness": result.tool_correctness,
                        "param_accuracy": result.param_accuracy,
                        "total_calls": result.total_calls,
                        "correct_calls": result.correct_calls,
                        "valid_param_calls": result.valid_param_calls,
                    }
                    for task_id, result in tue_by_task_results.items()
                }
            except Exception as e:
                logger.warning(f"Enhanced TUE computation failed, falling back to simple: {e}")
                # Fallback to original TUE calculation
                all_costs = [call["cost"] for call in all_tool_calls if call["cost"] > 0]
                all_latencies = [call["latency"] for call in all_tool_calls if call["latency"] > 0]
                cost_cap = np.percentile(all_costs, 95) if all_costs else 1.0
                latency_cap = np.percentile(all_latencies, 95) if all_latencies else 1.0
                cost_cap = max(cost_cap, 0.001)
                latency_cap = max(latency_cap, 0.001)
                # Fallback to simple TUE calculation
                num_total = len(all_tool_calls)
                num_correct = sum(1 for call in all_tool_calls if call.get("correct", False))
                num_valid_params = sum(1 for call in all_tool_calls if call.get("params_valid", False))
                T_correct = num_correct / num_total
                P_params = num_valid_params / num_total
                total_cost = sum(call.get("cost", 0.0) for call in all_tool_calls)
                C_cost = max(0.0, min(1.0, 1.0 - (total_cost / cost_cap))) if cost_cap > 0 else 1.0
                valid_latencies = [call.get("latency", 0.0) for call in all_tool_calls if call.get("latency", 0.0) > 0]
                if valid_latencies and latency_cap > 0:
                    avg_latency = sum(valid_latencies) / len(valid_latencies)
                    capped_latency = min(avg_latency, latency_cap)
                    L_latency = max(0.0, min(1.0, 1.0 - (capped_latency / latency_cap)))
                else:
                    L_latency = 1.0
                tue = 0.4 * T_correct + 0.25 * P_params + 0.2 * C_cost + 0.15 * L_latency
                
                # Extract basic metrics for fallback
                tue_correct_calls = sum(1 for call in all_tool_calls if call.get("correct", False))
                tue_valid_param_calls = sum(1 for call in all_tool_calls if call.get("params_valid", False))
                tue_tool_correctness = tue_correct_calls / len(all_tool_calls) if all_tool_calls else 0.0
                tue_param_accuracy = tue_valid_param_calls / len(all_tool_calls) if all_tool_calls else 0.0
                tue_by_task = {}
        else:
            tcrr = 0.0
            tue = 0.0
            tcrr_window_size = 3
            tcrr_total_calls = 0
            tcrr_redundant_calls = 0
            tcrr_by_task = {}
            tue_tool_correctness = 0.0
            tue_param_accuracy = 0.0
            tue_correct_calls = 0
            tue_valid_param_calls = 0
            tue_by_task = {}

        # Compute enhanced GSRT metrics
        try:
            from tau2.metrics.gsrt import compute_gsrt_enhanced_metrics

            judge_model = getattr(results.info, "gsrt_judge_llm", None) or "gpt-4o-mini"
            judge_args = getattr(results.info, "gsrt_judge_llm_args", None) or {
                "temperature": 0.0
            }
            
            gsrt_aggregate = compute_gsrt_enhanced_metrics(
                results.tasks, results.simulations, judge_model, judge_args
            )
            
            gsrt_median_ack = gsrt_aggregate.aggregate_median_ack
            gsrt_median_tool = gsrt_aggregate.aggregate_median_tool
            gsrt_median_outcome = gsrt_aggregate.aggregate_median_outcome
            gsrt_num_shifts = gsrt_aggregate.total_shifts
            gsrt_recovery_rate = gsrt_aggregate.overall_recovery_rate
            gsrt_transfer_rate = gsrt_aggregate.overall_transfer_rate
            gsrt_never_recovered_rate = gsrt_aggregate.never_recovered_rate
            gsrt_by_task = {
                task_id: {
                    "median_ack": task_result.median_gsrt_ack,
                    "median_tool": task_result.median_gsrt_tool,
                    "median_outcome": task_result.median_gsrt_outcome,
                    "total_shifts": task_result.total_shifts,
                    "recovery_rate": task_result.recovery_rate,
                    "transfer_rate": task_result.transfer_rate,
                }
                for task_id, task_result in gsrt_aggregate.by_task.items()
            }
        except Exception as e:
            logger.warning(f"Enhanced GSRT computation failed: {e}")
            gsrt_median_ack, gsrt_median_tool, gsrt_median_outcome = None, None, None
            gsrt_num_shifts = 0
            gsrt_recovery_rate, gsrt_transfer_rate, gsrt_never_recovered_rate = 0.0, 0.0, 0.0
            gsrt_by_task = {}

        return AgentMetrics(
            avg_reward=avg_reward,
            pass_hat_ks=pass_hat_ks,
            avg_agent_cost=avg_agent_cost,
            tsr=tsr,
            tue=tue,
            tcrr=tcrr,
            num_tool_calls=num_tool_calls,
            tsr_communicate_info=tsr_communicate_info,
            tsr_action=tsr_action,
            tsr_nl_assertion=tsr_nl_assertion,
            tsr_weights=tsr_weights,
            tsr_by_task=tsr_by_task,
            tcrr_window_size=tcrr_window_size,
            tcrr_total_calls=tcrr_total_calls,
            tcrr_redundant_calls=tcrr_redundant_calls,
            tcrr_by_task=tcrr_by_task,
            tue_tool_correctness=tue_tool_correctness,
            tue_param_accuracy=tue_param_accuracy,
            tue_correct_calls=tue_correct_calls,
            tue_valid_param_calls=tue_valid_param_calls,
            tue_by_task=tue_by_task,
            gsrt_median_ack=gsrt_median_ack,
            gsrt_median_tool=gsrt_median_tool,
            gsrt_median_outcome=gsrt_median_outcome,
            gsrt_num_shifts=gsrt_num_shifts,
            gsrt_recovery_rate=gsrt_recovery_rate,
            gsrt_transfer_rate=gsrt_transfer_rate,
            gsrt_never_recovered_rate=gsrt_never_recovered_rate,
            gsrt_by_task=gsrt_by_task,
        )
    except Exception as e:
        logger.warning(
            f"Full metrics computation failed: {e}. Falling back to simple computation."
        )
        return compute_metrics_simple(results)


def get_tasks_pass_hat_k(results: Results) -> pd.DataFrame:
    """
    Compute the pass^k for each k from 1 to the maximum number of trials.
    """
    df, max_k = get_metrics_df(results)
    df_pass_hat_k = get_tasks_pass_hat_k(results)
    df_pass_hat_k["num_actions"] = df.groupby("task_id").first()["task_num_actions"]
    df_pass_hat_k = df_pass_hat_k.sort_values(by="num_actions")
    return df_pass_hat_k


def prepare_dfs(results: Results) -> tuple[pd.DataFrame, pd.DataFrame]:
    df, max_k = get_metrics_df(results)
    df_pass_hat_k = get_tasks_pass_hat_k(results)
    df_pass_hat_k["num_actions"] = df.groupby("task_id").first()["task_num_actions"]
    df_pass_hat_k = df_pass_hat_k.sort_values(by="num_actions")
    return df, df_pass_hat_k


def display_metrics(metrics: AgentMetrics) -> None:
    print(f"🏆 Average reward: {metrics.avg_reward}")
    print("📈 Pass^k")
    for k, pass_hat_k in metrics.pass_hat_ks.items():
        print(f"  k={k}: {pass_hat_k}")
    print(f"💰 Average agent cost: {metrics.avg_agent_cost}")
    print()
    print("🎯 AgentChangeBench Metrics:")
    print(f"  📊 TSR (Task Success Rate): {metrics.tsr:.2%}")
    print(f"    💬 Communicate Info: {metrics.tsr_communicate_info:.2%} (weight: {metrics.tsr_weights.get('communicate_info', 0.5):.0%})")
    print(f"    ⚙️  Actions: {metrics.tsr_action:.2%} (weight: {metrics.tsr_weights.get('action', 0.3):.0%})")
    print(f"    📝 NL Assertions: {metrics.tsr_nl_assertion:.2%} (weight: {metrics.tsr_weights.get('nl_assertion', 0.2):.0%})")
    if metrics.tsr_by_task:
        print(f"    📋 Task-Level Breakdown: {len(metrics.tsr_by_task)} tasks")
    print(f"  ⚙️  TUE (Tool Usage Efficiency): {metrics.tue:.2%}")
    print(f"    🎯 Tool Correctness: {metrics.tue_tool_correctness:.2%} ({metrics.tue_correct_calls}/{metrics.num_tool_calls})")
    print(f"    📝 Parameter Accuracy: {metrics.tue_param_accuracy:.2%} ({metrics.tue_valid_param_calls}/{metrics.num_tool_calls})")
    if metrics.tue_by_task:
        print(f"    📋 Task-Level Breakdown: {len(metrics.tue_by_task)} tasks")
    print(f"  🔄 TCRR (Tool-Call Redundancy Ratio): {metrics.tcrr:.2%}")
    print(f"    📊 Redundant Calls: {metrics.tcrr_redundant_calls}/{metrics.tcrr_total_calls}")
    print(f"    🪟 Window Size: {metrics.tcrr_window_size} assistant turns")
    if metrics.tcrr_by_task:
        print(f"    📋 Task-Level Breakdown: {len(metrics.tcrr_by_task)} tasks")
    print(f"  🛠️  Total Tool Calls: {metrics.num_tool_calls}")
    print(f"  🔀 GSRT (Goal Shift Recovery Time):")
    if metrics.gsrt_num_shifts > 0:
        print(f"    📊 Goal Shifts Detected: {metrics.gsrt_num_shifts}")
        print(f"    📈 Recovery Rate: {metrics.gsrt_recovery_rate:.1%}")
        print(f"    🔄 Transfer Rate: {metrics.gsrt_transfer_rate:.1%}")
        print(f"    ❌ Never Recovered Rate: {metrics.gsrt_never_recovered_rate:.1%}")
        print(f"    Recovery Times by Variant:")
        if metrics.gsrt_median_ack is not None:
            print(f"      🎯 Acknowledgment: {metrics.gsrt_median_ack:.1f} turns (median)")
        if metrics.gsrt_median_tool is not None:
            print(f"      🛠️  Tool Usage: {metrics.gsrt_median_tool:.1f} turns (median)")
        if metrics.gsrt_median_outcome is not None:
            print(f"      ✅ Outcome Success: {metrics.gsrt_median_outcome:.1f} turns (median)")
        if metrics.gsrt_by_task:
            print(f"    📋 Task-Level Breakdown: {len(metrics.gsrt_by_task)} tasks with shifts")
    else:
        print(f"    ❌ No goal shifts detected")


if __name__ == "__main__":
    import argparse
    from pathlib import Path

    parser = argparse.ArgumentParser()
    parser.add_argument("--results", type=str, required=True)
    args = parser.parse_args()
    results = Results.load(Path(args.results))
    metrics = compute_metrics(results)
    display_metrics(metrics)
