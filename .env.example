ANTHROPIC_API_KEY=<your_key_here>
OPENAI_API_KEY=<your_key_here>

HF_HOME=/workspace/hf
HUGGING_FACE_HUB_TOKEN=hf_xxx

QWEN_MODEL=Qwen/Qwen3-30B-A3B-GPTQ-Int4
MISTRAL_MODEL=TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
DEEPSEEK_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-14B

MISTRAL_EXTRA_FLAGS="--quantization gptq --max-model-len 16384 --max-num-seqs 1 --max-num-batched-tokens 1024 --gpu-memory-utilization 0.70 --enforce-eager --swap-space 8"
QWEN_EXTRA_FLAGS="--quantization gptq --max-num-seqs 1 --gpu-memory-utilization 0.65 --max-model-len 12000 --swap-space 8 --enforce-eager"

DTYPE_MISTRAL=float16
DTYPE_QWEN=float16
DTYPE_DEEPSEEK=float16

QWEN_PORT=8005
MISTRAL_PORT=8006
DEEPSEEK_PORT=8007

#2x L40 GPUs
DEEPSEEK_FLAGS="--tensor-parallel-size 2 --max-model-len 65536 --gpu-memory-utilization 0.90 --max-num-seqs 4 --max-num-batched-tokens 8192 --enforce-eager --enable-chunked-prefill"
QWEN_FLAGS="--tensor-parallel-size 2 --max-model-len 32768 --gpu-memory-utilization 0.85 --max-num-seqs 2 --max-num-batched-tokens 4096 --enforce-eager"
MISTRAL_FLAGS="--tensor-parallel-size 2 --max-model-len 32768 --gpu-memory-utilization 0.85 --max-num-seqs 2 --max-num-batched-tokens 4096 --enforce-eager"

# Deepseek self-host
HOSTED_VLLM_API_BASE=https://<runpod_id>-8080.proxy.runpod.net/deepseek/v1
HOSTED_VLLM_API_KEY=dummy-key

# Mistral self-host
HOSTED_VLLM_API_BASE=http://<runpod_id>-8080.proxy.runpod.net/mistral/v1
HOSTED_VLLM_API_KEY=dummy-key